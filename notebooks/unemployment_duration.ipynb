{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unemployment duration\n",
    "\n",
    "We are interested in modeling unemployment duration. For that purpose consider the following “hazard function”:\n",
    "$$\n",
    "\\lambda\\left(t,X_i\\right)=\\frac{\\gamma_i\\alpha t^{\\alpha-1}}{1+\\gamma_it^{\\alpha}},\n",
    "\\quad\\gamma_i=\\exp\\left\\{ X_i\\beta\\right\\} ,\\quad i=\\overline{1,N},\n",
    "$$\n",
    "where $\\alpha$ and $\\beta$ are unknown parameters and $X_{i}$ are explanatory variables. If we define a variable $T_{i}$ as an unemployment duration, then the hazard function can be interpreted using the following definition:\n",
    "$$\n",
    "\\lambda\\left(t,X_i\\right)=\\lim_{h\\downarrow 0}\\frac{P\\left[\\left.t\\leq T_{i}<t+h\\right|T_{i}\\geq t,X_{i}\\right]}{h}.\n",
    "$$\n",
    "\n",
    "Given that, we can find the distribution of $\\log\\left(T_{i}\\right)$:\n",
    "\\begin{aligned}\n",
    "F\\left(\\exp\\left(t\\right)\\right)= & 1-\\exp\\left\\{ -\\int_{0}^{\\exp\\left(t\\right)}\\lambda\\left(s\\right)ds\\right\\} \\\\\n",
    "= & 1-\\exp\\left\\{ -\\int_{0}^{\\exp\\left(t\\right)}\\frac{\\gamma\\alpha s^{\\alpha-1}}{1+\\gamma s^{\\alpha}}ds\\right\\} \\\\\n",
    "= & 1-\\exp\\left\\{ -\\int_{0}^{\\exp\\left(t\\right)}\\frac{d\\left(\\gamma s^{\\alpha}\\right)}{1+\\gamma s^{\\alpha}}\\right\\} \\\\\n",
    "= & 1-\\exp\\left\\{ -\\log\\left(1+\\gamma\\exp\\left(\\alpha t\\right)\\right)\\right\\} \\\\\n",
    "= & 1-\\frac{1}{1+\\gamma\\exp\\left(\\alpha t\\right)}\\\\\n",
    "= & \\frac{\\gamma\\exp\\left(\\alpha t\\right)}{1+\\gamma\\exp\\left(\\alpha t\\right)}.\n",
    "\\end{aligned}\n",
    "This is logistic distribution.\n",
    "\n",
    "Alternatively,\n",
    "\\begin{aligned}F\\left(\\exp\\left(t\\right)\\right)= & \\left[1+\\exp\\left(-\\alpha t\\right)/\\gamma\\right]^{-1}\\\\\n",
    "= & \\left[1+\\exp\\left\\{ -\\log\\left(\\gamma\\right)-\\alpha t\\right\\} \\right]^{-1}\\\\\n",
    "= & \\left[1+\\exp\\left\\{ -\\frac{t+\\log\\left(\\gamma\\right)/\\alpha}{1/\\alpha}\\right\\} \\right]^{-1}\n",
    "\\end{aligned}\n",
    "Hence,\n",
    "$$\n",
    "E\\left[\\log T\\right]=-\\frac{\\log\\left(\\gamma\\right)}{\\alpha}\n",
    "=-\\frac{X_{i}\\beta}{\\alpha},\\quad V\\left[\\log T\\right]=\\frac{\\pi^{2}}{3\\alpha^{2}}.\n",
    "$$\n",
    "\n",
    "Conditional density of $\\log T_{i}$ is\n",
    "$$\n",
    "f\\left(Y_{i}\\left|X_{i},\\theta\\right.\\right)\n",
    "=s_{i}^{-1}\\exp\\left\\{ -\\frac{Y_{i}-\\mu_{i}}{s_{i}}\\right\\}\n",
    "\\left(1+\\exp\\left\\{ -\\frac{Y_{i}-\\mu_{i}}{s_{i}}\\right\\} \\right)^{-2},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mu_{i}=-\\frac{X_{i}\\beta}{\\alpha},\\quad s_{i}=\\frac{\\pi}{\\sqrt{3}\\alpha}.\n",
    "$$\n",
    "The conditional log-likelihood function is\n",
    "$$\n",
    "L\\left(\\theta\\right)=\\frac{1}{n}\\sum_{i=1}^{n}l_{i}\\left(\\theta\\right),\n",
    "$$\n",
    "where\n",
    "\\begin{aligned}l_{i}\\left(\\theta\\right)\n",
    "= & \\log f\\left(Y_{i}\\left|X_{i},\\theta\\right.\\right)\\\\\n",
    "= & -\\log\\left(s_{i}\\right)-\\frac{Y_{i}-\\mu_{i}}{s_{i}}-2\\log\\left(1+\\exp\\left\\{ -\\frac{Y_{i}-\\mu_{i}}{s_{i}}\\right\\} \\right).\\\\\n",
    "= & \\log\\left(\\pi^{-1}\\sqrt{3}\\right)+\\log\\left(\\alpha\\right)-\\pi^{-1}\\sqrt{3}\\left(\\alpha Y_{i}+X_{i}\\beta\\right)-2\\log\\left(1+\\exp\\left\\{ -\\pi^{-1}\\sqrt{3}\\left(\\alpha Y_{i}+X_{i}\\beta\\right)\\right\\} \\right).\n",
    "\\end{aligned}\n",
    "\n",
    "The score function is\n",
    "$$\n",
    "s_{i}\\left(\\theta\\right)=\\frac{\\partial}{\\partial\\theta}l_{i}\\left(\\theta\\right)\n",
    "=\\left[\\frac{\\partial}{\\partial\\alpha}l_{i}\\left(\\theta\\right),\\frac{\\partial}{\\partial\\beta^{\\prime}}l_{i}\\left(\\theta\\right)\\right].\n",
    "$$\n",
    "The first term is\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\alpha}l_{i}\\left(\\theta\\right)\n",
    "=\\alpha^{-1}-\\pi^{-1}\\sqrt{3}Y_{i}+2\\pi^{-1}\\sqrt{3}Y_{i}\\left(1+\\gamma_{i}\\right)^{-1},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\gamma_{i}=\\exp\\left\\{ \\pi^{-1}\\sqrt{3}\\left(\\alpha Y_{i}+X_{i}\\beta\\right)\\right\\}\n",
    "$$\n",
    "The second is\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\beta^{\\prime}}l_{i}\\left(\\theta\\right)\n",
    "=-\\pi^{-1}\\sqrt{3}X_{i}+2\\pi^{-1}\\sqrt{3}X_{i}\\left(1+\\gamma_{i}\\right)^{-1}.\n",
    "$$\n",
    "\n",
    "The Hessian function is\n",
    "$$\n",
    "H_{i}\\left(\\theta\\right)\n",
    "=\\frac{\\partial^{2}}{\\partial\\theta\\partial\\theta^{\\prime}}l_{i}\\left(\\theta\\right)\n",
    "=\\left[\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}}{\\partial\\alpha^{2}}l_{i}\\left(\\theta\\right) & \\frac{\\partial^{2}}{\\partial\\alpha\\partial\\beta^{\\prime}}l_{i}\\left(\\theta\\right)\\\\\n",
    "\\frac{\\partial^{2}}{\\partial\\alpha\\partial\\beta}l_{i}\\left(\\theta\\right) & \\frac{\\partial^{2}}{\\partial\\beta^{\\prime}\\partial\\beta}l_{i}\\left(\\theta\\right)\n",
    "\\end{array}\\right].\n",
    "$$\n",
    "The first term diagonal term is\n",
    "$$\n",
    "\\frac{\\partial^{2}}{\\partial\\alpha^{2}}l_{i}\\left(\\theta\\right)\n",
    "=-\\alpha^{-2}-6\\pi^{-2}Y_{i}^{2}\\gamma_{i}\\left(1+\\gamma_{i}\\right)^{-2}.\n",
    "$$\n",
    "The second diagonal term is\n",
    "$$\n",
    "\\frac{\\partial^{2}}{\\partial\\beta\\partial\\beta^{\\prime}}l_{i}\\left(\\theta\\right)\n",
    "=-6\\pi^{-2}X_{i}X_{i}^{\\prime}\\gamma_{i}\\left(1+\\gamma_{i}\\right)^{-2}.\n",
    "$$\n",
    "The off-diagonal term is\n",
    "$$\n",
    "\\frac{\\partial^{2}}{\\partial\\alpha\\partial\\beta}l_{i}\\left(\\theta\\right)\n",
    "=-6\\pi^{-2}Y_{i}X_{i}\\gamma_{i}\\left(1+\\gamma_{i}\\right)^{-2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import minimize, check_grad\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "pd.set_option('float_format', '{:6.3f}'.format)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3343, 11)\n",
      "   spell  censor1  censor2  censor3  censor4  age   ui  reprate  disrate  \\\n",
      "0      5        1        0        0        0   41   no    0.179    0.045   \n",
      "1     13        1        0        0        0   30  yes    0.520    0.130   \n",
      "2     21        1        0        0        0   36  yes    0.204    0.051   \n",
      "3      3        1        0        0        0   26  yes    0.448    0.112   \n",
      "4      9        0        0        1        0   22  yes    0.320    0.080   \n",
      "\n",
      "   logwage  tenure  \n",
      "0    6.896       3  \n",
      "1    5.288       6  \n",
      "2    6.767       1  \n",
      "3    5.979       3  \n",
      "4    6.315       0  \n"
     ]
    }
   ],
   "source": [
    "zf = zipfile.ZipFile('../data/UnempDur.zip')\n",
    "# Get the name of the csv file\n",
    "name = zf.namelist()[0]\n",
    "# Set up reader\n",
    "data = zf.open(name)\n",
    "# Read csv file\n",
    "raw = pd.read_csv(data).iloc[:, 1:]\n",
    "\n",
    "print(raw.shape)\n",
    "print(raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3343,) (3343, 10)\n"
     ]
    }
   ],
   "source": [
    "def dummy(x):\n",
    "    if x == 'yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "raw['ui'] = raw['ui'].apply(dummy)\n",
    "Y = np.log(raw['spell'].values)\n",
    "X = raw.drop('spell', axis=1).values\n",
    "\n",
    "print(Y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticDuration(object):\n",
    "    \n",
    "    \"\"\"The class for logistic log-duration model.\"\"\"\n",
    "    \n",
    "    def __init__(self, Y, X):\n",
    "        # Initialize the data\n",
    "        self.Y, self.X = Y, X\n",
    "        # Compute number of observations and explanatory variables.\n",
    "        self.N, self.K = X.shape\n",
    "        # Note that the number of parameters is K+1!\n",
    "        self.K +=1\n",
    "    \n",
    "    def pmean(self):\n",
    "        \"\"\"Returns (N,) array.\"\"\"\n",
    "        return -(self.X.dot(self.beta)) / self.alpha\n",
    "    \n",
    "    def pvar(self):\n",
    "        \"\"\"Returns float.\"\"\"\n",
    "        return np.pi/3**.5 / self.alpha\n",
    "    \n",
    "    def loglikelihood(self, theta):\n",
    "        \"\"\"Negative of the log-likelihood function.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : (K,) array\n",
    "            Parameters of the model\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "        \n",
    "        \"\"\"\n",
    "        self.alpha, self.beta = theta[0], theta[1:]\n",
    "        normalized = (self.Y - self.pmean()) / self.pvar()\n",
    "        f = -np.log(self.pvar()) - normalized - 2 * np.log(1 + np.exp(-normalized))\n",
    "        return -f.mean()\n",
    "    \n",
    "    def gamma(self):\n",
    "        \"\"\"Helper function.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (N,) array\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.exp(3**.5/np.pi * (self.alpha * Y + self.X.dot(self.beta)))\n",
    "    \n",
    "    def dl_da(self):\n",
    "        \"\"\"Derivative with respect to alpha.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (N,) array\n",
    "        \n",
    "        \"\"\"\n",
    "        return 1/self.alpha - Y * 3**.5/np.pi * (1 - 2 / (1 + self.gamma()))\n",
    "        \n",
    "    def dl_db(self):\n",
    "        \"\"\"Derivative with respect to beta.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (K, N) array\n",
    "        \n",
    "        \"\"\"\n",
    "        return -X.T * 3**.5/np.pi * (1 - 2 / (1 + self.gamma()))\n",
    "    \n",
    "    def score(self):\n",
    "        \"\"\"Score fucntion of the log-likelihood.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (K, N) array\n",
    "        \n",
    "        \"\"\"\n",
    "        return -np.vstack([self.dl_da(), self.dl_db()])\n",
    "    \n",
    "    def jac(self, theta):\n",
    "        \"\"\"Gradient of the objective function.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : (K,) array\n",
    "            Parameters of the model\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        (K,) array\n",
    "        \n",
    "        \"\"\"\n",
    "        self.alpha, self.beta = theta[0], theta[1:]\n",
    "        return self.score().mean(1)\n",
    "    \n",
    "    def d2l_da2(self):\n",
    "        \"\"\"Second derivative with respect to alpha.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (N,) array\n",
    "        \n",
    "        \"\"\"\n",
    "        return -1/self.alpha**2 - 6/np.pi**2 * self.Y**2 * self.gamma() / (1 + self.gamma())**2\n",
    "    \n",
    "    def d2l_dadb(self):\n",
    "        \"\"\"Second derivative with respect to alpha and beta.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (K, N) array\n",
    "        \n",
    "        \"\"\"\n",
    "        return - 6/np.pi**2 * self.X.T * self.Y * self.gamma() / (1 + self.gamma())**2\n",
    "    \n",
    "    def d2l_db2(self):\n",
    "        \"\"\"Second derivative with respect to alpha and beta.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (N, K, K) array\n",
    "        \n",
    "        \"\"\"\n",
    "        XX = self.X.T[np.newaxis,:,:] * self.X.T[:,np.newaxis,:]\n",
    "        return - 6/np.pi**2 * XX * self.gamma() / (1 + self.gamma())**2\n",
    "    \n",
    "    def hess(self, theta):\n",
    "        \"\"\"Hessian of the objective function.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : (K,) array\n",
    "            Parameters of the model\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        (K, K) array\n",
    "        \n",
    "        \"\"\"\n",
    "        self.alpha, self.beta = theta[0], theta[1:]\n",
    "        mat = np.empty((self.K, self.K))\n",
    "        mat[0, 0] = self.d2l_da2().mean()\n",
    "        mat[1:, 0] = self.d2l_dadb().mean(1)\n",
    "        mat[0, 1:] = mat[1:, 0]\n",
    "        mat[1:, 1:] = self.d2l_db2().mean(-1)\n",
    "        return -mat\n",
    "    \n",
    "    def info_matrix(self):\n",
    "        \"\"\"Variance of the scores.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (K, K) array\n",
    "        \n",
    "        \"\"\"\n",
    "        score = self.score()\n",
    "        return score.dot(score.T) / self.N\n",
    "\n",
    "    def theta_var(self, version):\n",
    "        \"\"\"Variance matrix of parameter estimates.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        version : str\n",
    "            The choice of estimator. Choose among ['scores', 'hess', 'sandwich']\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        (K, K) array\n",
    "        \n",
    "        \"\"\"\n",
    "        if version == 'scores':\n",
    "            return np.linalg.inv(self.info_matrix()) / model.N\n",
    "        elif version == 'hess':\n",
    "            return np.linalg.inv(self.hess(self.theta_hat)) / model.N\n",
    "        elif version == 'sandwich':\n",
    "            Hinv = np.linalg.inv(self.hess(self.theta_hat)) / model.N\n",
    "            return Hinv.dot(self.info_matrix()).dot(Hinv) * model.N\n",
    "        else:\n",
    "            raise Exception(\"Choose version among ['scores', 'hess', 'sandwich']!\")\n",
    "        \n",
    "    def std_err(self, version):\n",
    "        \"\"\"Standard errors of parameter estimates.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        version : str\n",
    "            The choice of estimator. Choose among ['scores', 'hess', 'sandwich']\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        (K,) array\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.diag(self.theta_var(version)) ** .5\n",
    "    \n",
    "    def t_stats(self, version):\n",
    "        \"\"\"T-statistics of parameter estimates.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        version : str\n",
    "            The choice of estimator. Choose among ['scores', 'hess', 'sandwich']\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        (K,) array\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.theta_hat / self.std_err(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to use this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model instance, set the initial parameter guess, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticDuration(Y, X)\n",
    "theta_start = np.ones(X.shape[1] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between analytic and numerical gradient = 5.01210924177e-07\n"
     ]
    }
   ],
   "source": [
    "print('The difference between analytic and numerical gradient =',\n",
    "      check_grad(model.loglikelihood, model.jac, theta_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate model parameters\n",
    "\n",
    "Use different methods, with and without Jacobian and Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.98  0.11  0.03 -0.29 -2.1  -0.02 -3.03 -1.24 -0.54 -0.31  0.01]\n",
      "[ 3.98  0.11  0.03 -0.29 -2.1  -0.02 -3.03 -1.24 -0.54 -0.31  0.01]\n",
      "[ 3.98  0.11  0.03 -0.3  -2.11 -0.02 -3.03 -1.25 -0.53 -0.31  0.01]\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "res1 = minimize(model.loglikelihood, theta_start, method='BFGS')\n",
    "res2 = minimize(model.loglikelihood, theta_start, method='BFGS', jac=model.jac)\n",
    "res3 = minimize(model.loglikelihood, theta_start, method='dogleg', jac=model.jac, hess=model.hess)\n",
    "\n",
    "print(res1.x)\n",
    "print(res2.x)\n",
    "print(res3.x)\n",
    "\n",
    "# Check that the results are close\n",
    "print(np.allclose(res1.x, res2.x, rtol=1e-3))\n",
    "print(np.allclose(res1.x, res3.x, rtol=1e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time all these procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 518 ms per loop\n",
      "10 loops, best of 3: 156 ms per loop\n",
      "10 loops, best of 3: 143 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit minimize(model.loglikelihood, theta_start, method='BFGS')\n",
    "%timeit minimize(model.loglikelihood, theta_start, method='BFGS', jac=model.jac)\n",
    "%timeit minimize(model.loglikelihood, theta_start, method='dogleg', jac=model.jac, hess=model.hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect results into one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat    stderr                  tstats                  theta_hat\n",
      "version scores   hess sandwich  scores    hess sandwich          \n",
      "0        0.071  0.057    0.048  55.707  70.266   83.117     3.979\n",
      "1        0.288  0.338    0.399   0.383   0.326    0.276     0.110\n",
      "2        0.312  0.366    0.430   0.085   0.073    0.062     0.027\n",
      "3        0.300  0.346    0.402  -0.979  -0.851   -0.732    -0.294\n",
      "4        0.289  0.336    0.396  -7.285  -6.261   -5.316    -2.104\n",
      "5        0.006  0.006    0.006  -4.193  -4.033   -3.869    -0.024\n",
      "6        0.115  0.128    0.145 -26.414 -23.643  -20.944    -3.030\n",
      "7        0.448  0.484    0.531  -2.778  -2.570   -2.343    -1.244\n",
      "8        0.819  0.840    0.866  -0.660  -0.644   -0.624    -0.541\n",
      "9        0.057  0.062    0.068  -5.362  -4.942   -4.483    -0.305\n",
      "10       0.010  0.011    0.012   1.232   1.122    1.019     0.012\n"
     ]
    }
   ],
   "source": [
    "model.theta_hat = res1.x\n",
    "versions = ['scores', 'hess', 'sandwich']\n",
    "stderr, tstats = [], []\n",
    "for version in versions:\n",
    "    stderr.append(model.std_err(version))\n",
    "    tstats.append(model.t_stats(version))\n",
    "\n",
    "stderr = pd.DataFrame(np.vstack(stderr).T, columns=versions)\n",
    "tstats = pd.DataFrame(np.vstack(tstats).T, columns=versions)\n",
    "df = pd.concat([stderr, tstats], keys=['stderr', 'tstats'], names=['stat', 'version'], axis=1)\n",
    "df['theta_hat'] = model.theta_hat\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
